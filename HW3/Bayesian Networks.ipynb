{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOTYeFsP08Pw7mvU5TkJ2Lw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Bayesian Networks"],"metadata":{"id":"kP7MW8T_-N9d"}},{"cell_type":"markdown","source":["In this programming assignment, we will investigate the structure of the binarized MNIST dataset of handwritten digits using Bayesian networks. The dataset contains images of handwritten digits with dimensions $28 \\times 28$ (784) pixels. Consider the Bayesian network in Figure 1 . The network contains two layers of variables. The variables in the bottom layer, $X_{1: 784}$ denote the pixel values of the flattened image. The variables in the top layer, $Z_{1}$ and $Z_{2}$, are referred to as latent variables, because the value of these variables will not be explicitly provided by the data and will have to be inferred."],"metadata":{"id":"gDjos4zM-O08"}},{"cell_type":"markdown","source":["![Figure1](./Images/fig1.png)"],"metadata":{"id":"f7pV2Ieo-R_0"}},{"cell_type":"markdown","source":["The Bayesian network specifies a joint probability distribution over binary images and latent variables $p\\left(Z_{1}, Z_{2}, X_{1: 784}\\right)$. The model is trained so that the marginal probability of the manifest variables, $p\\left(x_{1: 784}\\right)=\\sum_{z_{1}, z_{2}} p\\left(z_{1}, z_{2}, x_{1: 784}\\right)$ is high on images that look like digits, and low for other images. \n","\n","For this programming assignment, we provide a pretrained model trained_mnist_model. The starter code loads this model and provides functions to directly access the conditional probability tables. Further, we simplify the problem by discretizing the latent and manifest variables such that $\\operatorname{Val}\\left(Z_{1}\\right)=\\operatorname{Val}\\left(Z_{2}\\right)=\\{-3,-2.75, \\ldots, 2.75,3\\}$ and $\\operatorname{Val}\\left(X_{j}\\right)=\\{0,1\\}$, i.e., the image is binary."],"metadata":{"id":"uMBrr5rl-Tak"}},{"cell_type":"markdown","source":["### 1. \n","How many values can the random vector $X_{1: 784}$ take, i.e., how many different $28 \\times 28$ binary images are there?\n","\n","How many parameters would you need to specify an arbitrary probability distribution over all possible $28 \\times 28$ binary images? (5 points)"],"metadata":{"id":"pd5TTzCb-VFt"}},{"cell_type":"code","source":["print('There can be at most 2^(28*28) different binary images')"],"metadata":{"id":"_k1iE8c3-XOs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('There can be at most 25*25*28*28 parameters')"],"metadata":{"id":"e_jhD8GZ-YQ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run below codes to load the network and its functions."],"metadata":{"id":"TMEkMUut-ZvM"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pickle as pkl\n","from scipy.io import loadmat\n","NUM_PIXELS = 28*28"],"metadata":{"id":"lZMbwBee-axe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_p_z1(z1_val):\n","    '''\n","    Helper. Computes the prior probability for variable z1 to take value z1_val.\n","    P(Z1=z1_val)\n","    '''\n","    return bayes_net['prior_z1'][z1_val]\n","def get_p_z2(z2_val):\n","    '''\n","    Helper. Computes the prior probability for variable z2 to take value z2_val.\n","    P(Z2=z2_val)\n","    '''\n","    return bayes_net['prior_z2'][z2_val]"],"metadata":{"id":"ebBqJmIv-cAU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_p_xk_cond_z1_z2(z1_val, z2_val, k):\n","    '''\n","    Helper. Computes the conditional probability that variable xk assumes value 1\n","    given that z1 assumes value z1_val and z2 assumes value z2_val\n","    P(Xk = 1 | Z1=z1_val , Z2=z2_val)\n","    '''\n","    return bayes_net['cond_likelihood'][(z1_val, z2_val)][0, k-1]\n","\n","\n","def get_p_x_cond_z1_z2(z1_val, z2_val):\n","    '''\n","    Computes the conditional probability of the entire vector x for x = 1,\n","    given that z1 assumes value z1_val and z2 assumes value z2_val\n","    '''\n","    pk = np.zeros(NUM_PIXELS)\n","    for i in range(NUM_PIXELS):\n","        pk[i] = get_p_xk_cond_z1_z2(z1_val, z2_val, i+1)\n","    return pk"],"metadata":{"id":"gQarpeVg-c80"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_model(model_file):\n","    '''\n","    Loads a default Bayesian network with latent variables (in this case, a\n","    variational autoencoder)\n","    '''\n","    with open('Helper_codes/trained_mnist_model', 'rb') as infile:\n","        cpts = pkl.load(infile, encoding='bytes')\n","    model = {}\n","    model['prior_z1'] = cpts[0]\n","    model['prior_z2'] = cpts[1]\n","    model['cond_likelihood'] = cpts[2]\n","    return model"],"metadata":{"id":"fQr2nKeR-d4N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["global disc_z1, disc_z2\n","n_disc_z = 25\n","disc_z1 = np.linspace(-3, 3, n_disc_z)\n","disc_z2 = np.linspace(-3, 3, n_disc_z)\n","\n","global bayes_net\n","bayes_net = load_model('Helper_codes/trained_mnist_model')"],"metadata":{"id":"qUel1vFg-e6E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.\n","Produce 5 samples from the joint probability distribution $\\left(z_{1}, z_{2}, x_{1: 784}\\right) \\sim p\\left(Z_{1}, Z_{2}, X_{1: 784}\\right)$, and plot the corresponding images (values of the pixel variables). (7 points)"],"metadata":{"id":"XSVs-Nu4-Wa8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2-J-W0I-NAV"},"outputs":[],"source":["fig = plt.figure(figsize=(15, 15))\n","p = np.zeros(5)\n","for i in range(5):\n","    z1 = np.random.choice(disc_z1)\n","    z2 = np.random.choice(disc_z2)\n","    Conditional_Probability = get_p_x_cond_z1_z2(z1, z2)\n","    Vector_Sample = np.random.binomial(1, 0.5, 784)\n","    Matrix_Sample = np.reshape(Vector_Sample,[28,28])\n","    p[i] = get_p_z1(z1) * get_p_z2(z2)\n","    for j in range(784):\n","        if Vector_Sample[i] == 1:\n","            p[i] *= Conditional_Probability[i]\n","        else:\n","            p[i] *= (1-Conditional_Probability[i])\n","    fig.add_subplot(1, 5, i+1)\n","    plt.imshow(Matrix_Sample,cmap='gray', vmin=0, vmax=1)\n","plt.show()\n","for i in range(5):\n","    print('Joint probability distribution of sample %d is %.20f' % (i, p[i]))  "]},{"cell_type":"markdown","source":["### 3.\n","\n","For each possible value of\n","$$\n","\\left(\\bar{z}_{1}, \\bar{z}_{2}\\right) \\in\\{-3,-2.75, \\ldots, 2.75,3\\} \\times\\{-3,-2.75, \\ldots, 2.75,3\\}\n","$$\n","compute the conditional expectation $E\\left[X_{1: 784} \\mid Z_{1}, Z_{2}=\\left(\\bar{z}_{1}, \\bar{z}_{2}\\right)\\right] .$ This is the expected image corresponding to each possible value of the latent variables $Z_{1}, Z_{2} .$ Plot the images on on a $2 \\mathrm{D}$ grid where the grid axes correspond to $Z_{1}$ and $Z_{2}$ respectively. What is the intuitive role of the $Z_{1}, Z_{2}$ variables in this model? (8 points)"],"metadata":{"id":"jqSGhxDp-3OV"}},{"cell_type":"code","source":["fig, axs = plt.subplots(len(disc_z1), len(disc_z2),figsize=(20, 20))\n","for i in range(len(disc_z1)):\n","    z1 = disc_z1[i]\n","    for j in range(len(disc_z2)): \n","        z2 = disc_z2[j]\n","        Conditional_Probability = get_p_x_cond_z1_z2(z1, z2)\n","        Matrix_Sample = np.reshape(Conditional_Probability,[28,28])\n","        axs[i, j].imshow(Matrix_Sample,cmap='gray', vmin=0, vmax=1)\n"],"metadata":{"id":"-mMO2kRl-5L1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mat = loadmat('Helper_codes/testval.mat')\n","val_data = mat['val_x']\n","test_data = mat['test_x']"],"metadata":{"id":"uILUgYJ7-7DG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.\n","You are given a validation and a test dataset. In the test dataset, some images are \"real\" handwritten digits, and some are anomalous (corrupted images). We would like to use our Bayesian network to distinguish real images from the anomalous ones. Intuitively, our Bayesian network should assign low probability to corrupted images and high probability to the real ones, and we can use this for classification. To do this, we first compute the average marginal log-likelihood,\n","$$\n","\\log p\\left(x_{1: 784}\\right)=\\log \\sum_{z_{1}} \\sum_{z_{2}} p\\left(z_{1}, z_{2}, x_{1: 784}\\right)\n","$$\n","on the validation dataset, and the standard deviation (again, standard deviation over the validation set). Consider a simple prediction rule where images with marginal log-likelihood, $\\log p\\left(x_{1: 784}\\right)$, outside three standard deviations of the average marginal log-likelihood are classified as corrupted. Classify images in the test set as corrupted or real using this rule. Then plot a histogram of the marginal log-likelihood for the images classified as \"real\". Plot a separate histogram of the marginal log-likelihood for the images classified as \"corrupted\". (15 points)"],"metadata":{"id":"VNMr-CA3-8oV"}},{"cell_type":"code","source":["k1 = np.size(test_data,axis=0)\n","Marginal_Likelihood = np.zeros(k1)\n","for i in range(k1):\n","    Vector = test_data[i]\n","    for j1 in range(len(disc_z1)):\n","        z1 = disc_z1[j1]\n","        for j2 in range(len(disc_z2)):\n","            z2 = disc_z2[j]       \n","            Conditional_Probability = get_p_x_cond_z1_z2(z1, z2)\n","            p = np.log(get_p_z1(z1)) + np.log(get_p_z2(z2))\n","            for k in range(784):\n","                if Vector[k] == 1:\n","                    p +=  np.log(Conditional_Probability[k])\n","                else:\n","                    p +=  np.log((1-Conditional_Probability[k]))\n","            Marginal_Likelihood[i] += p\n","            \n","My_Mean = np.mean(Marginal_Likelihood)\n","My_Std  = np.std(Marginal_Likelihood)\n","\n","print(My_Mean,My_Std)"],"metadata":{"id":"6UWTwEVn-8Gm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k2 = np.size(val_data,axis=0)\n","Val_labels = np.zeros(k2)\n","Marginal_Likelihood_Val = np.zeros(k2)\n","Real_Image_Marginal_Likelihood = []\n","Corrupted_Image_Marginal_Likelihood = []\n","for i in range(k2):    \n","    Vector = val_data[i]\n","    for j1 in range(len(disc_z1)):\n","        z1 = disc_z1[j1]\n","        for j2 in range(len(disc_z2)):\n","            z2 = disc_z2[j]  \n","            Conditional_Probability = get_p_x_cond_z1_z2(z1, z2)\n","            p = np.log(get_p_z1(z1)) + np.log(get_p_z2(z2))\n","            for k in range(784):\n","                if Vector[k] == 1:\n","                    p += np.log(Conditional_Probability[k])\n","                else:\n","                    p += np.log((1-Conditional_Probability[k]))\n","            Marginal_Likelihood_Val[i] += p\n","            \n","    print(Marginal_Likelihood_Val[i], np.abs(Marginal_Likelihood_Val[i]-My_Mean), 3 * My_Std)\n","    if np.abs(Marginal_Likelihood_Val[i]-My_Mean) > 3 * My_Std:\n","        Val_labels[i] = 0\n","        Corrupted_Image_Marginal_Likelihood.append(Marginal_Likelihood_Val[i])\n","    else:\n","        Val_labels[i] = 1\n","        Real_Image_Marginal_Likelihood.append(Marginal_Likelihood_Val[i])"],"metadata":{"id":"OERjhjyL-_Xl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n","axs[0].hist(Corrupted_Image_Marginal_Likelihood, density=True)\n","axs[1].hist(Real_Image_Marginal_Likelihood, density=True)"],"metadata":{"id":"ORZ3uwfV_Apm"},"execution_count":null,"outputs":[]}]}