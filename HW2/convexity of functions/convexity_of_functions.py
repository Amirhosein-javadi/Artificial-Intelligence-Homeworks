# -*- coding: utf-8 -*-
"""convexity of functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/171Dx2m6_GdXxkN0aUMqHM-7nowg4Vfp0

# Q1 (30 Points + 5 Extra Points)

<font size=4>
Author: Reza Amini
			<br/>
                <font color=red>
Please run all the cells.
     </font>
</font>
                <br/>
    </div>

## Libraries
"""

import numpy as np
from typing import Tuple
from matplotlib import pyplot as plt
import math

"""<div>
	<font size=3>
        <hr/>
            In this part of the question, we want to implement the functions and draw diagrams and discuss about the convexity of functions
           <hr/>
    </font>
</div>

## Functions

a)
$f_1(x) = \frac{x^2 cos(\frac{x}{10}) - x}{100} \quad $ where $\quad x \in [80, 110]$
<br/>
b)
$f_2(x) = \log (\sqrt{sin \frac{x}{20}}) \quad $ where $\quad x \in [1, 60]$
<br/>
c)
$f_3(x) = \log (\cos(x) + \frac{45}{x}) \quad $ where $\quad x \in [1, 45]$

<div>
    <font size=3>
In the first step, implement the functions.
     </font>
</div>
"""

def f_1(x):
    return ((x**2)*np.cos(x/10)-x)/100

def f_2(x):
    return np.log(sin(x/20)**0.5)

def f_3(x):
    return np.log(cos(x)+45/x)

"""<div>
    <font size=3>
    Now implement a function to plot the mathematical functions
     </font>
</div>
"""

def draw(func, x_range):
    y = f_1(x_range)
    plt.plot(x_range,y)
    plt.grid(True)
    plt.xlabel('x')
    plt.ylabel('y')
    plt.show()
    pass

draw(f_1, np.arange(80, 110 + 1))

draw(f_2, np.arange(1, 60 + 1))

draw(f_3, np.arange(1, 45 + 1))

"""<div>
    <font size=3>
    Discuss the Convexity of each function according to the diagrams
     </font>
</div>

<font color=blue>
The only convex function is first function. <br/>
Second function and third function have local maximums, so they can't be a convex function.
</font>

<div>
    <font size=3>
what method do you suggest to find the maximum in the second function?
     </font>
</div>

<font color=blue>
    We need to add some randomization and allow some bad moves to escape local minimums. (Ideas like Hill-Climbing with Both Random Walk & Random
Sampling and Simulated Annealing or ) <br/>
    Also, We can run the algorithm for couple of times. Since the initial point can make a huge effect on final result.
</font>

<div>
    <font size=3>
        <hr/>
        <ul>
            In this part, we want to obtain a minimum of one of the above functions with the gradiant descent algorithm
        </ul>  
        <hr/>
    </font>
</div>

## Gradiant descent

<div>
    <font size=3>
    Complete gradiant descent function and calculate the minimum of function (a).
     </font>
</div>
"""

def gradiant_descent(func, initial_point: float, learning_rate: float, max_iterations: int):
    counter = 1
    x = initial_point
    h = 0.01
    while counter <= max_iterations:
        Derivative = (func(x+h)-func(x))/h
        x = x - learning_rate*Derivative
        counter += 1
    return x

gradiant_descent(
    func=f_1, 
    initial_point=np.random.uniform(80, 110), 
    learning_rate=0.05, 
    max_iterations=1000
)

"""<div>
    <font size=3>
In the last part, we want to investigate the effect of the learning rate hyperparameter on the gradiant descent algorithm with a new function     
    </font>
</div>

## Function

$f(x_1, x_2) = 2x_1^2 + 3x_2^2 - 4x_1x_2 - 50x + 6y$
"""

def f(x_1, x_2):
    return 2 * x_1**2 + 3 * x_2**2  - 4 * x_1 * x_2 - 50 * x_1 + 6 * x_2

"""<div>
    <font size=3>
Implement gradiant descent function for two-dimensional functions, in such a way that it gives sequence of x_1 and x_2 in the output ()
    </font>
</div>
"""

def gradiant_descent(func, initial_point: Tuple, learning_rate: float, threshold: float, max_iterations: int):
    x_1_sequence = [initial_point[0]]
    x_2_sequence = [initial_point[1]]
    counter = 1
    while counter<max_iterations :
        new_x_1,new_x_2 = update_points(func, x_1_sequence[-1], x_2_sequence[-1], learning_rate)
        x_1_sequence.append(new_x_1)
        x_2_sequence.append(new_x_2)
        if new_x_1>threshold:
            return x_1_sequence, x_2_sequence
        if new_x_2>threshold:
            return x_1_sequence, x_2_sequence
        counter += 1  
    return x_1_sequence, x_2_sequence

def update_points(func, x_1, x_2, learning_rate):
    h = 0.01
    Derivative_x_1 = (func(x_1+h,x_2)-func(x_1,x_2))/h
    Derivative_x_2 = (func(x_1,x_2+h)-func(x_1,x_2))/h
    new_x_1 = x_1 - learning_rate * Derivative_x_1
    new_x_2 = x_2 - learning_rate * Derivative_x_2
    return new_x_1, new_x_2

def draw_points_sequence(func, x_1_sequence, x_2_sequence):
    X1, X2 = np.meshgrid(np.linspace(-100.0, 100.0, 100), np.linspace(-100.0, 100.0, 100))
    Y = func(X1, X2)
    cp = plt.contour(X1, X2, Y, colors='black', linestyles='dashed', linewidths=1)
    plt.clabel(cp, inline=1, fontsize=10)
    cp = plt.contourf(X1, X2, Y, )
    plt.xlabel('X1')
    plt.ylabel('X2')
    plt.scatter(x_1_sequence, x_2_sequence, s=10, c="y")
    plt.show()

"""<div>
    <font size=3>
    Run the gradiant descent algorithm for each of the learning rate values and use the above function to draw diagram
    </font>
</div>
"""

initial_point = (-100, 100)
learning_rates = [0.01, 0.05, 0.19, 0.4]
threshold = 100
max_iterations = 1000

x_1_sequence, x_2_sequence = gradiant_descent(f,initial_point,learning_rates[0],threshold,max_iterations)
draw_points_sequence(f, x_1_sequence, x_2_sequence)

x_1_sequence, x_2_sequence = gradiant_descent(f,initial_point,learning_rates[1],threshold,max_iterations)
draw_points_sequence(f, x_1_sequence, x_2_sequence)

x_1_sequence, x_2_sequence = gradiant_descent(f,initial_point,learning_rates[2],threshold,max_iterations)
draw_points_sequence(f, x_1_sequence, x_2_sequence)

x_1_sequence, x_2_sequence = gradiant_descent(f,initial_point,learning_rates[3],threshold,max_iterations)
draw_points_sequence(f, x_1_sequence, x_2_sequence)

"""<div>
    <font size=3>
    explain your result comprehensively from the charts, 
    </font>
</div>

<font color=blue>
    Although our function is convex, a large learning rate doesn't allow us to reach the local minimum. <br/>
    The third learning rate makes the fastest convergence and the first learning rate makes the slowest convergence. <br/>
    The lower the learning rate is, the slowest algorithm converges and the probability of convergence is higher.
    
</font>
"""