{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q1 (30 Points + 5 Extra Points)"
      ],
      "metadata": {
        "id": "-behvKbCIdFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W4w4F_WhIfI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=4>\n",
        "Author: Reza Amini\n",
        "\t\t\t<br/>\n",
        "                <font color=red>\n",
        "Please run all the cells.\n",
        "     </font>\n",
        "</font>\n",
        "                <br/>\n",
        "    </div>"
      ],
      "metadata": {
        "id": "z-Mx4vyrIeWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "eAhollkaIgkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Tuple\n",
        "from matplotlib import pyplot as plt\n",
        "import math"
      ],
      "metadata": {
        "id": "oVFsqU0MIiTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "\t<font size=3>\n",
        "        <hr/>\n",
        "            In this part of the question, we want to implement the functions and draw diagrams and discuss about the convexity of functions\n",
        "           <hr/>\n",
        "    </font>\n",
        "</div>"
      ],
      "metadata": {
        "id": "zaaG_2HmIjfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "q6Lj0gANIlPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)\n",
        "$f_1(x) = \\frac{x^2 cos(\\frac{x}{10}) - x}{100} \\quad $ where $\\quad x \\in [80, 110]$\n",
        "<br/>\n",
        "b)\n",
        "$f_2(x) = \\log (\\sqrt{sin \\frac{x}{20}}) \\quad $ where $\\quad x \\in [1, 60]$\n",
        "<br/>\n",
        "c)\n",
        "$f_3(x) = \\log (\\cos(x) + \\frac{45}{x}) \\quad $ where $\\quad x \\in [1, 45]$"
      ],
      "metadata": {
        "id": "bYK0c5DyImg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "    <font size=3>\n",
        "In the first step, implement the functions.\n",
        "     </font>\n",
        "</div>"
      ],
      "metadata": {
        "id": "IKUPdl35IoL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f_1(x):\n",
        "    return ((x**2)*np.cos(x/10)-x)/100"
      ],
      "metadata": {
        "id": "vJM-fQ6DIpUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f_2(x):\n",
        "    return np.log(sin(x/20)**0.5)"
      ],
      "metadata": {
        "id": "V5Y3FcYXIqW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f_3(x):\n",
        "    return np.log(cos(x)+45/x)"
      ],
      "metadata": {
        "id": "GTnIZ52xIrVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "    <font size=3>\n",
        "    Now implement a function to plot the mathematical functions\n",
        "     </font>\n",
        "</div>"
      ],
      "metadata": {
        "id": "gUMkHID_IsaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw(func, x_range):\n",
        "    y = f_1(x_range)\n",
        "    plt.plot(x_range,y)\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.show()\n",
        "    pass"
      ],
      "metadata": {
        "id": "QV7C83FaItc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw(f_1, np.arange(80, 110 + 1))"
      ],
      "metadata": {
        "id": "zOG1vBQ9IuaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw(f_2, np.arange(1, 60 + 1))"
      ],
      "metadata": {
        "id": "AQh6glWkIvhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw(f_3, np.arange(1, 45 + 1))"
      ],
      "metadata": {
        "id": "zVz81Z7bIwlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "    <font size=3>\n",
        "    Discuss the Convexity of each function according to the diagrams\n",
        "     </font>\n",
        "</div>"
      ],
      "metadata": {
        "id": "8tDNJ432I1jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=blue>\n",
        "The only convex function is first function. <br/>\n",
        "Second function and third function have local maximums, so they can't be a convex function.\n",
        "</font>"
      ],
      "metadata": {
        "id": "Uk3J0LR0I0X2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "    <font size=3>\n",
        "what method do you suggest to find the maximum in the second function?\n",
        "     </font>\n",
        "</div>"
      ],
      "metadata": {
        "id": "Y_tkr00yI4_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=blue>\n",
        "    We need to add some randomization and allow some bad moves to escape local minimums. (Ideas like Hill-Climbing with Both Random Walk & Random\n",
        "Sampling and Simulated Annealing or ) <br/>\n",
        "    Also, We can run the algorithm for couple of times. Since the initial point can make a huge effect on final result.\n",
        "</font>"
      ],
      "metadata": {
        "id": "bo_NOVWWI6pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "    <font size=3>\n",
        "        <hr/>\n",
        "        <ul>\n",
        "            In this part, we want to obtain a minimum of one of the above functions with the gradiant descent algorithm\n",
        "        </ul>  \n",
        "        <hr/>\n",
        "    </font>\n",
        "</div>"
      ],
      "metadata": {
        "id": "MK2IPvrCI8Lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradiant descent"
      ],
      "metadata": {
        "id": "T3FaMzXUI97e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "    <font size=3>\n",
        "    Complete gradiant descent function and calculate the minimum of function (a).\n",
        "     </font>\n",
        "</div>"
      ],
      "metadata": {
        "id": "ns9mQ-LCI_vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradiant_descent(func, initial_point: float, learning_rate: float, max_iterations: int):\n",
        "    counter = 1\n",
        "    x = initial_point\n",
        "    h = 0.01\n",
        "    while counter <= max_iterations:\n",
        "        Derivative = (func(x+h)-func(x))/h\n",
        "        x = x - learning_rate*Derivative\n",
        "        counter += 1\n",
        "    return x"
      ],
      "metadata": {
        "id": "YS8DQQPoJAoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradiant_descent(\n",
        "    func=f_1, \n",
        "    initial_point=np.random.uniform(80, 110), \n",
        "    learning_rate=0.05, \n",
        "    max_iterations=1000\n",
        ")"
      ],
      "metadata": {
        "id": "Vzy_g4u9JBc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "    <font size=3>\n",
        "In the last part, we want to investigate the effect of the learning rate hyperparameter on the gradiant descent algorithm with a new function     \n",
        "    </font>\n",
        "</div>"
      ],
      "metadata": {
        "id": "9mWnxFwQJCof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function"
      ],
      "metadata": {
        "id": "HDxK9MPmJD-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f(x_1, x_2) = 2x_1^2 + 3x_2^2 - 4x_1x_2 - 50x + 6y$"
      ],
      "metadata": {
        "id": "UjYiR5MUJFL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x_1, x_2):\n",
        "    return 2 * x_1**2 + 3 * x_2**2  - 4 * x_1 * x_2 - 50 * x_1 + 6 * x_2"
      ],
      "metadata": {
        "id": "rKngoSs1JGHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "    <font size=3>\n",
        "Implement gradiant descent function for two-dimensional functions, in such a way that it gives sequence of x_1 and x_2 in the output ()\n",
        "    </font>\n",
        "</div>"
      ],
      "metadata": {
        "id": "wS2cEg9JJIfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradiant_descent(func, initial_point: Tuple, learning_rate: float, threshold: float, max_iterations: int):\n",
        "    x_1_sequence = [initial_point[0]]\n",
        "    x_2_sequence = [initial_point[1]]\n",
        "    counter = 1\n",
        "    while counter<max_iterations :\n",
        "        new_x_1,new_x_2 = update_points(func, x_1_sequence[-1], x_2_sequence[-1], learning_rate)\n",
        "        x_1_sequence.append(new_x_1)\n",
        "        x_2_sequence.append(new_x_2)\n",
        "        if new_x_1>threshold:\n",
        "            return x_1_sequence, x_2_sequence\n",
        "        if new_x_2>threshold:\n",
        "            return x_1_sequence, x_2_sequence\n",
        "        counter += 1  \n",
        "    return x_1_sequence, x_2_sequence\n",
        "\n",
        "def update_points(func, x_1, x_2, learning_rate):\n",
        "    h = 0.01\n",
        "    Derivative_x_1 = (func(x_1+h,x_2)-func(x_1,x_2))/h\n",
        "    Derivative_x_2 = (func(x_1,x_2+h)-func(x_1,x_2))/h\n",
        "    new_x_1 = x_1 - learning_rate * Derivative_x_1\n",
        "    new_x_2 = x_2 - learning_rate * Derivative_x_2\n",
        "    return new_x_1, new_x_2"
      ],
      "metadata": {
        "id": "PeAj9kPKJJeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_points_sequence(func, x_1_sequence, x_2_sequence):\n",
        "    X1, X2 = np.meshgrid(np.linspace(-100.0, 100.0, 100), np.linspace(-100.0, 100.0, 100))\n",
        "    Y = func(X1, X2)\n",
        "    cp = plt.contour(X1, X2, Y, colors='black', linestyles='dashed', linewidths=1)\n",
        "    plt.clabel(cp, inline=1, fontsize=10)\n",
        "    cp = plt.contourf(X1, X2, Y, )\n",
        "    plt.xlabel('X1')\n",
        "    plt.ylabel('X2')\n",
        "    plt.scatter(x_1_sequence, x_2_sequence, s=10, c=\"y\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "RYgzbJM3JKZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "    <font size=3>\n",
        "    Run the gradiant descent algorithm for each of the learning rate values and use the above function to draw diagram\n",
        "    </font>\n",
        "</div>"
      ],
      "metadata": {
        "id": "eG8knZ3lJLiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_point = (-100, 100)\n",
        "learning_rates = [0.01, 0.05, 0.19, 0.4]\n",
        "threshold = 100\n",
        "max_iterations = 1000"
      ],
      "metadata": {
        "id": "imkvgXfCJMjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_1_sequence, x_2_sequence = gradiant_descent(f,initial_point,learning_rates[0],threshold,max_iterations)\n",
        "draw_points_sequence(f, x_1_sequence, x_2_sequence)"
      ],
      "metadata": {
        "id": "1osHmzg4JNmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_1_sequence, x_2_sequence = gradiant_descent(f,initial_point,learning_rates[1],threshold,max_iterations)\n",
        "draw_points_sequence(f, x_1_sequence, x_2_sequence)"
      ],
      "metadata": {
        "id": "YR-qyBVfJOoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_1_sequence, x_2_sequence = gradiant_descent(f,initial_point,learning_rates[2],threshold,max_iterations)\n",
        "draw_points_sequence(f, x_1_sequence, x_2_sequence)"
      ],
      "metadata": {
        "id": "dr7rRaL5JPiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_1_sequence, x_2_sequence = gradiant_descent(f,initial_point,learning_rates[3],threshold,max_iterations)\n",
        "draw_points_sequence(f, x_1_sequence, x_2_sequence)"
      ],
      "metadata": {
        "id": "dwke3LwbJQtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "    <font size=3>\n",
        "    explain your result comprehensively from the charts, \n",
        "    </font>\n",
        "</div>"
      ],
      "metadata": {
        "id": "f58gTPpBJSIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=blue>\n",
        "    Although our function is convex, a large learning rate doesn't allow us to reach the local minimum. <br/>\n",
        "    The third learning rate makes the fastest convergence and the first learning rate makes the slowest convergence. <br/>\n",
        "    The lower the learning rate is, the slowest algorithm converges and the probability of convergence is higher.\n",
        "    \n",
        "</font>"
      ],
      "metadata": {
        "id": "P5UD1HxJJTR3"
      }
    }
  ]
}